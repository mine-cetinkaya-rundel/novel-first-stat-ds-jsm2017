---
title: "Three Methods for Statistical Inference"
author: "Ben Baumer"
date: "July 31, 2017"
output:
  rmdshower::shower_presentation:
    self_contained: false
    katex: true
    ratio: 16x10
abstract: "Statistics educators---imbued with improved computing power---have advocated for a greater emphasis on randomization and simulation-based techniques for statistical inference in recent years. While these ideas are not new, the traditional treatment of inference in introductory statistics courses has focused on methods that approximate the sampling distribution of a statistic with a probability distribution. We describe an approach to teaching inference in an introductory statistics course---for students who primarily major in the sciences---that emphasizes randomization and simulation-based approaches, briefly discusses but largely glosses over mathematical approaches using probability theory, and treats normal-based approximations as an alternative technique. The overall conceptual goal is to understand the sampling distribution of the test statistic: the three approaches are just means to that end. "
---

## Three Methods for Statistical Inference { .white }

<img src="http://1.bp.blogspot.com/-ueOnJA9cGJk/VHWza-0saaI/AAAAAAAAP4g/WTbdP2qNSaE/s1920/monkey-wallpaper-hd.jpg" class="cover height">

</br></br></br>
<p class="white">
Ben Baumer</br>
JSM Baltimore</br>
July 31st, 2017</br>
(https://github.com/mine-cetinkaya-rundel/novel-first-stat-ds-jsm2017)
</p>

```{r, echo=FALSE}
knitr::opts_chunk$set(fig.height = 3.5)
```

# Why am I talking about inference?

## Inference

<div class="double">
- [ASA Statement on P-Values and Statistical Significance](http://amstat.tandfonline.com/doi/full/10.1080/00031305.2016.1154108)
- [Symposium on Statistical Inference](https://ww2.amstat.org/meetings/ssi/2017/)

<img src="https://ww2.amstat.org/meetings/ssi/2017/images/logo.png"width="400px">
</div>

## It's not just me...


<div class="double">
<p class"double-flow">
- Brad Efron</br>
<img src="https://web.stanford.edu/~hastie/CASI/images/jacket_wave_cropped.jpg" height="300px">
</p>

<p class"double-flow">
- John Wellner
<img src="https://ww2.amstat.org/meetings/jsm/2017/images/featured/FeaturedSpeakers-web_Wellner.png" height="300px">
</p>
</div>

## The shoulders of greats

<div class="double">
<p class="double-flow">
- George Cobb
- Nick Horton
- Katherine Halvorsen
- Andrew Bray

<img src="https://www.causeweb.org/sbi/wp-content/uploads/2015/03/GCobb.png" width="150px">
<img src="https://www.amherst.edu/system/files/styles/original/private/media/photo/1545596.jpg" width="150px">
<img src="http://magazine.amstat.org/wp-content/uploads/2017/02/BOD-Katherine-Halvorsen150x200.jpg" width="150px">
<img src="https://andrewpbray.github.io/abray-pic2.JPG" width="150px">
</p>
</div>


## About Smith


<div class="double">
<img src="https://i.forbesimg.com/media/lists/colleges/smith-college_416x416.jpg" alt="smith" width="300px">

<p class="double-flow">
- liberal arts college for women
- new major in [Statistical & Data Sciences](http://www.smith.edu/sds/major.php)
    - $\geq$ *3* courses in statistics
    - $\geq$ *2* courses in programming
    - $\geq$ *1* course in data science
- no modeling or inference in data science
</p>
</div>

## SDS 220

<div class="double">
<p class="double-flow">
- most advanced intro stats course
- calc or discrete math pre-req
- 5 credits: 3 lectures, 1 lab
- mostly STEM majors
    - 40% EGR and BIO majors
</p>

<p class="double-flow">
<img src="https://images-na.ssl-images-amazon.com/images/I/414KqCmzuhL._SX258_BO1,204,203,200_.jpg" alt="OpenIntro" width="300px">
</div>

## Three Methods to construct null distribution

> 1. Simulation: use computer to *simulate* 
> 2. Exact: use math to *compute*
> 3. Approximation: use statistical theory to *approximate*


## When I took stats

- ~~Simulation~~
- ~~Exact~~ (in probability class)
- Approximation


# An example:</br>one proportion

## Method \#1: simulation


```{r, message=FALSE, warning=FALSE}
library(tidyverse)

outcomes <- data_frame(candidate = c("clinton", "trump"))
p_0 <- 1/2

# http://www.cnn.com/election/results/exit-polls
n <- 246

sim <- outcomes %>%
  oilabs::rep_sample_n(size = n, replace = TRUE, reps = 10000) %>%
  group_by(replicate) %>%
  summarize(N = n(), 
            clinton_votes = sum(candidate == "clinton")) %>%
  mutate(clinton_pct = clinton_votes / N)
```

## Method \#1: simulation, observed

```{r}
# observed proportion
p_hat <- data_frame(clinton_pct = 0.5092953)

observed <- ggplot(data = p_hat) + 
  geom_vline(aes(xintercept = clinton_pct), 
             color = "gold", size = 2, linetype = 2) + 
  scale_x_continuous(limits = c(0.45, 0.55))
```

## Method \#1: simulation, plot

```{r, message=FALSE}
observed + 
  geom_density(data = sim, aes(x = clinton_pct))
```

## Method \#2: exact

- Let $X \sim Bernoulli(p_0)$, then
    - $\mathbb{E}[X] = p_0, \qquad Var[X] = p_0 (1 - p_0)$
    
- Let $Z = \frac{X_1 + \cdots + X_n}{n}$, then
    - $\mathbb{E}[Z] = p_0, \qquad Var[Z] = \frac{p_0 (1 - p_0)}{n}$
    - for later, $sd(Z) = \sqrt{\frac{p_0 (1 - p_0)}{n}}$



## Method \#2: exact, plot

```{r, fig.height=3}
dbinom_p <- function (x, size, prob, log = FALSE) {
  n * dbinom(round(x * size), size, prob, log)
}
observed +
  stat_function(fun = dbinom_p, args = c(size = n, prob = p_0))
```

## Method \#3: approximation

- For $np > 10$ and $n(1-p) > 10$,
$$\frac{1}{n} \cdot Binomial(n, p_0) \approx Normal \left( p_0, \sqrt{\frac{p_0 (1 - p_0)}{n}} \right) $$

```{r}
se_p0 <- sqrt(p_0 * (1-p_0) / n)
```

## Method \#3: approximation, plot

```{r}
observed +
  stat_function(fun = dnorm, args = c(mean = p_0, sd = se_p0))
```

## Three methods comparison, plot

```{r, warning=FALSE}
observed +
  geom_density(data = sim, aes(x = clinton_pct)) + 
  stat_function(fun = dbinom_p, args = c(size = n, prob = p_0), color = "cyan") +
  stat_function(fun = dnorm, args = c(mean = p_0, sd = se_p0), color = "magenta")
```

## Three methods comparison, table

--------------------------------------------------------------------------------
             Simulation                    Exact             Approximation
----------- --------------------- --------------------- ------------------------
Assumptions     independence           independence          independence
                                     probability model       $np > 10$, etc.

  Pros          no math               exact solution          uses normal
                flexible                                   approx. usually good
                                                         no CPU required (sort of)

   Cons        requires CPU              HARD!              more assumptions
               non-deterministic    not always known            not exact 
--------------------------------------------------------------------------------

## Example \#2: two categorical variables

- Simulation
    - randomization test
- Exact
    - Fisher's exact test, hypergeometric 
- Approximation
    - $\chi^2$-test of independence

## DataCamp

<div class="double">
<p class="double-flow">
- [Statistics with R](https://www.datacamp.com/tracks/statistics-with-r) skills track
- 4 courses available now
- 4 more courses available soon

</p>
<p class="double-flow">
  <img src="datacamp.png" width="300px">
</p>
</div>


## Pictures { .fullpage }

<img src="statistical_inference.jpg" class="white" width="600px">

# Unification?

## Common setup with `infer`

```{r, error=TRUE}
# devtools::install_github("andrewpbray/infer")
library(infer)

fake_null <- data_frame(
  candidate = rep(c("clinton", "trump"), each = n/2)
)

setup <- fake_null %>%
  specify(response = candidate) %>%
  hypothesize(null = "point", p = c("clinton" = 0.5, "trump" = 0.5))
```

## Method \#1 with `infer`

```{r, fig.height=3}
setup %>%
  generate(reps = 1000, type = "simulate") %>%
  calculate(stat = "prop") %>%
  visualize() + 
  geom_vline(data = p_hat, aes(xintercept = clinton_pct), 
             color = "gold", size = 2, linetype = 2)
```

## Method \#3 with `infer`

```{r, eval=FALSE}
setup %>%
  calculate(stat = "prop") %>%
  visualize() + 
  geom_vline(data = p_hat, aes(xintercept = clinton_pct), 
             color = "gold", size = 2, linetype = 2)
```

## Thank you! {.fullpage}

<img src="https://media.giphy.com/media/aAW7yJ4m7YCti/giphy.gif" width="400px">

</br></br></br></br></br></br>
(https://github.com/mine-cetinkaya-rundel/novel-first-stat-ds-jsm2017)
