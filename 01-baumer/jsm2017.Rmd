---
title: "Three Methods for Statistical Inference"
author: "Ben Baumer"
date: "August 1, 2017"
output:
  rmdshower::shower_presentation:
    self_contained: false
    katex: true
    ratio: 16x10
abstract: "Statistics educators---imbued with improved computing power---have advocated for a greater emphasis on randomization and simulation-based techniques for statistical inference in recent years. While these ideas are not new, the traditional treatment of inference in introductory statistics courses has focused on methods that approximate the sampling distribution of a statistic with a probability distribution. We describe an approach to teaching inference in an introductory statistics course---for students who primarily major in the sciences---that emphasizes randomization and simulation-based approaches, briefly discusses but largely glosses over mathematical approaches using probability theory, and treats normal-based approximations as an alternative technique. The overall conceptual goal is to understand the sampling distribution of the test statistic: the three approaches are just means to that end. "
---

## Three Methods for Statistical Inference { .white }

<img src="http://1.bp.blogspot.com/-ueOnJA9cGJk/VHWza-0saaI/AAAAAAAAP4g/WTbdP2qNSaE/s1920/monkey-wallpaper-hd.jpg" class="cover height">

</br></br></br>
<p class="white">
Ben Baumer</br>
JSM Baltimore</br>
August 1st, 2017</br>
(https://github.com/mine-cetinkaya-rundel/novel-first-stat-ds-jsm2017)
</p>

```{r, echo=FALSE}
knitr::opts_chunk$set(fig.height = 3.5)
```

# Why am I talking about inference?

## Inference

<div class="double">
- [ASA Statement on P-Values and Statistical Significance](http://amstat.tandfonline.com/doi/full/10.1080/00031305.2016.1154108)
- [Symposium on Statistical Inference](https://ww2.amstat.org/meetings/ssi/2017/)

<img src="https://ww2.amstat.org/meetings/ssi/2017/images/logo.png"width="400px">
</div>

## It's not just me...


<div class="double">
<p class"double-flow">
- Brad Efron & Trevor Hastie</br>
<img src="https://web.stanford.edu/~hastie/CASI/images/jacket_wave_cropped.jpg" height="300px">
</p>

<p class"double-flow">
- Jon Wellner: [Teaching Statistics in the Age of Data Science](https://ww2.amstat.org/meetings/jsm/2017/onlineprogram/AbstractDetails.cfm?abstractid=322379) 
<img src="https://ww2.amstat.org/meetings/jsm/2017/images/featured/FeaturedSpeakers-web_Wellner.png" height="200px">
</p>
</div>

## The shoulders of giants

<div class="double">
<p class="double-flow">
- Katherine Halvorsen
- Nick Horton
- George Cobb
- Andrew Bray

<img src="http://magazine.amstat.org/wp-content/uploads/2017/02/BOD-Katherine-Halvorsen150x200.jpg" width="150px">
<img src="https://www.amherst.edu/system/files/styles/original/private/media/photo/1545596.jpg" width="150px">
<img src="https://www.causeweb.org/sbi/wp-content/uploads/2015/03/GCobb.png" width="150px">
<img src="https://andrewpbray.github.io/abray-pic2.JPG" width="150px">
</p>
</div>


## About Smith


<div class="double">
<img src="https://i.forbesimg.com/media/lists/colleges/smith-college_416x416.jpg" alt="smith" width="300px">

<p class="double-flow">
- liberal arts college for women
- new major in [Statistical & Data Sciences](http://www.smith.edu/sds/major.php)
    - $\geq$ *3* courses in statistics
    - $\geq$ *2* courses in programming
    - $\geq$ *1* course in data science
- no modeling or inference in data science
</p>
</div>

## SDS 220

<div class="double">
<p class="double-flow">
- most advanced intro stats course
- calc or discrete math pre-req
- 5 credits: 3 lectures, 1 lab
- mostly STEM majors
    - 40% EGR and BIO majors
</p>

<p class="double-flow">
<img src="https://images-na.ssl-images-amazon.com/images/I/414KqCmzuhL._SX258_BO1,204,203,200_.jpg" alt="OpenIntro" width="300px">
</div>

## Three Methods to construct null distribution

> 1. Simulation: use computer to *simulate* 
> 2. Exact: use math to *compute*
> 3. Approximation: use statistical theory to *approximate*


## When I took stats

> 1. ~~Simulation~~
> 2. ~~Exact~~ (in probability class)
> 3. Approximation


# An example:</br>one proportion

## Method \#1: simulation


```{r, message=FALSE, warning=FALSE}
library(tidyverse)

outcomes <- data_frame(candidate = c("clinton", "trump"))
p_0 <- 1/2

# http://www.cnn.com/election/results/exit-polls
n <- 246

sim <- outcomes %>%
  oilabs::rep_sample_n(size = n, replace = TRUE, reps = 10000) %>%
  group_by(replicate) %>%
  summarize(N = n(), 
            clinton_votes = sum(candidate == "clinton")) %>%
  mutate(clinton_pct = clinton_votes / N)
```

## Method \#1: simulation, observed

```{r}
# observed proportion
p_hat <- data_frame(clinton_pct = 0.5092953)

# what we know from data
observed <- ggplot(data = p_hat) + 
  geom_vline(aes(xintercept = clinton_pct), 
             color = "gold", size = 2, linetype = 2) + 
  scale_x_continuous(limits = c(0.4, 0.6))
```

## Method \#1: simulation, plot

```{r, message=FALSE, warning=FALSE}
observed + 
  geom_density(data = sim, aes(x = clinton_pct))
```

## Method \#2: exact

- Let $X \sim Bernoulli(p_0)$, then
    - $\mathbb{E}[X] = p_0, \qquad Var[X] = p_0 (1 - p_0)$
    
- Let $Z = \frac{X_1 + \cdots + X_n}{n}$, then
    - $\mathbb{E}[Z] = p_0, \qquad Var[Z] = \frac{p_0 (1 - p_0)}{n}$
    - for later, $sd(Z) = \sqrt{\frac{p_0 (1 - p_0)}{n}}$



## Method \#2: exact, plot

```{r, fig.height=3}
dbinom_p <- function (x, size, prob, log = FALSE) {
  n * dbinom(round(x * size), size, prob, log)
}
observed +
  stat_function(fun = dbinom_p, args = c(size = n, prob = p_0))
```

## Method \#3: approximation

- For $np > 10$ and $n(1-p) > 10$,
$$\frac{1}{n} \cdot Binomial(n, p_0) \approx Normal \left( p_0, \sqrt{\frac{p_0 (1 - p_0)}{n}} \right) $$

```{r}
se_p0 <- sqrt(p_0 * (1-p_0) / n)
```

## Method \#3: approximation, plot

```{r}
observed +
  stat_function(fun = dnorm, args = c(mean = p_0, sd = se_p0))
```

## Three methods comparison, plot

```{r, warning=FALSE}
observed +
  geom_density(data = sim, aes(x = clinton_pct)) + 
  stat_function(fun = dbinom_p, args = c(size = n, prob = p_0), color = "cyan") +
  stat_function(fun = dnorm, args = c(mean = p_0, sd = se_p0), color = "magenta")
```

## Three methods comparison, table

--------------------------------------------------------------------------------
             Simulation                    Exact             Approximation
----------- --------------------- --------------------- ------------------------
Assumptions     independence           independence          independence
                                     probability model       $np > 10$, etc.

  Pros         no math required        exact solution          uses normal
                  flexible                                 approx. usu. good
                                                             no CPU required

   Cons        requires computer    usu. HARD to derive!        more assumptions
               non-deterministic     not always known            not exact 
--------------------------------------------------------------------------------

## Example \#2: two categorical variables

- Simulation
    - randomization test
- Exact
    - Fisher's exact test, hypergeometric 
- Approximation
    - $\chi^2$-test of independence

# Unification?

## DataCamp

<div class="double">
<p class="double-flow">
- [Statistics with R](https://www.datacamp.com/tracks/statistics-with-r) skills track
- 4 courses available now
- 4 more courses available soon

</p>
<p class="double-flow">
  <img src="datacamp.png" width="300px">
</p>
</div>


## Pictures { .fullpage }

<img src="statistical_inference.jpg" class="white" width="600px">

## Common setup with `infer`

```{r, error=TRUE}
# devtools::install_github("andrewpbray/infer")
library(infer)

fake_null <- data_frame(
  candidate = rep(c("clinton", "trump"), each = n/2)
)

# independent of three methods
setup <- fake_null %>%
  specify(response = candidate) %>%
  hypothesize(null = "point", p = c("clinton" = 0.5, "trump" = 0.5))
```

## Method \#1 with `infer`

```{r, fig.height=3}
setup %>%
  generate(reps = 10000, type = "simulate") %>%
  calculate(stat = "prop") %>%
  visualize() + 
  geom_vline(data = p_hat, aes(xintercept = clinton_pct), 
             color = "gold", size = 2, linetype = 2)
```

## Method \#2 with `infer`

```{r, eval=FALSE}
# not implemented yet; compare to binom.test()
setup %>%
  calculate(stat = "prop") %>%
  visualize() + 
  geom_vline(data = p_hat, aes(xintercept = clinton_pct), 
             color = "gold", size = 2, linetype = 2)
```

## Method \#3 with `infer`

```{r, eval=FALSE}
# not implemented yet; compare to prop.test()
setup %>%
  calculate(stat = "prop") %>%
  visualize() + 
  geom_vline(data = p_hat, aes(xintercept = clinton_pct), 
             color = "gold", size = 2, linetype = 2)
```

## Thank you!

<div class="double">
<p class="double-flow">
<img src="https://media.giphy.com/media/aAW7yJ4m7YCti/giphy.gif" width="400px">
</p>

<p class="double-flow">
<img src="https://image.flaticon.com/icons/png/512/27/27630.png" width="100px"> [bbaumer@smith.edu](mailto:bbaumer@smith.edu)
</br>
<img src="https://www.aha.io/assets/github.7433692cabbfa132f34adb034e7909fa.png" width="100px"> [beanumber](https://github.com/mine-cetinkaya-rundel/novel-first-stat-ds-jsm2017)
</br>
<img src="http://www.tecnologia.com.pt/wp-content/uploads/2016/01/twitter-logo-2.png" width="100px"> [\@BaumerBen](https://twitter.com/BaumerBen)
</p>
</div>