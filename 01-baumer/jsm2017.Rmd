---
title: "Three Methods for Statistical Inference"
author: "Ben Baumer"
date: "July 31, 2017"
output:
  rmdshower::shower_presentation:
    self_contained: false
    katex: true
    ratio: 16x10
abstract: "Statistics educators---imbued with improved computing power---have advocated for a greater emphasis on randomization and simulation-based techniques for statistical inference in recent years. While these ideas are not new, the traditional treatment of inference in introductory statistics courses has focused on methods that approximate the sampling distribution of a statistic with a probability distribution. We describe an approach to teaching inference in an introductory statistics course---for students who primarily major in the sciences---that emphasizes randomization and simulation-based approaches, briefly discusses but largely glosses over mathematical approaches using probability theory, and treats normal-based approximations as an alternative technique. The overall conceptual goal is to understand the sampling distribution of the test statistic: the three approaches are just means to that end. "
---

## Three Methods for Statistical Inference { .white }

<img src="http://1.bp.blogspot.com/-ueOnJA9cGJk/VHWza-0saaI/AAAAAAAAP4g/WTbdP2qNSaE/s1920/monkey-wallpaper-hd.jpg" class="cover height">

</br></br></br>
<p class="white">
Ben Baumer</br>
JSM Baltimore</br>
July 31st, 2017</br>
(https://github.com/mine-cetinkaya-rundel/novel-first-stat-ds-jsm2017)
</p>

```{r, echo=FALSE}
knitr::opts_chunk$set(fig.height = 3.5)
```

## Three Methods to construct null distribution

> 1. Simulation: use computer to *simulate* 
> 2. Exact: use math to *compute*
> 3. Approximation: use statistical theory to *approximate*

# An example:</br>one proportion

## Method \#1: simulation


```{r, message=FALSE}
library(tidyverse)

outcomes <- data_frame(candidate = c("clinton", "trump"))
p_0 <- 1/2

# http://www.cnn.com/election/results/exit-polls
n <- 2458

sim <- outcomes %>%
  oilabs::rep_sample_n(size = n, replace = TRUE, reps = 1000) %>%
  group_by(replicate) %>%
  summarize(N = n(), 
            clinton_votes = sum(candidate == "clinton")) %>%
  mutate(clinton_pct = clinton_votes / N)
```

## Method \#1: simulation, observed

```{r}
# observed proportion
p_hat <- data_frame(clinton_pct = 0.5092953)

observed <- ggplot(data = p_hat) + 
  geom_vline(aes(xintercept = clinton_pct), 
             color = "gold", size = 2, linetype = 2) + 
  scale_x_continuous(limits = c(0.45, 0.55))
```

## Method \#1: simulation, plot

```{r, message=FALSE}
observed + 
  geom_density(data = sim, aes(x = clinton_pct))
```

## Method \#2: exact

- Let $X \sim Bernoulli(p_0)$, then
    - $\mathbb{E}[X] = p_0, \qquad Var[X] = p_0 (1 - p_0)$
    
- Let $Z = \frac{X_1 + \cdots + X_n}{n}$, then
    - $\mathbb{E}[Z] = p_0, \qquad Var[Z] = \frac{p_0 (1 - p_0)}{n}$
    - for later, $sd(Z) = \sqrt{\frac{p_0 (1 - p_0)}{n}}$



## Method \#2: exact, plot

```{r, fig.height=3}
dbinom_p <- function (x, size, prob, log = FALSE) {
  n * dbinom(round(x * size), size, prob, log)
}
observed +
  stat_function(fun = dbinom_p, args = c(size = n, prob = p_0))
```

## Method \#3: approximation

- For $np > 10$ and $n(1-p) > 10$,
$$\frac{1}{n} \cdot Binomial(n, p_0) \approx Normal \left( p_0, \sqrt{\frac{p_0 (1 - p_0)}{n}} \right) $$

```{r}
se_p0 <- sqrt(p_0 * (1-p_0) / n)
```

## Method \#3: approximation, plot

```{r}
observed +
  stat_function(fun = dnorm, args = c(mean = p_0, sd = se_p0))
```

## Three methods comparison, plot

```{r}
observed +
  geom_density(data = sim, aes(x = clinton_pct)) + 
  stat_function(fun = dbinom_p, args = c(size = n, prob = p_0), color = "cyan") +
  stat_function(fun = dnorm, args = c(mean = p_0, sd = se_p0), color = "magenta")
```

## Three methods comparison, table

--------------------------------------------------------------------------------
             Simulation                    Exact             Approximation
----------- --------------------- --------------------- ------------------------
Assumptions     independence           independence          independence
                                     probability model       $np > 10$, etc.

  Pros          no math               exact solution          uses normal
                flexible                                   approx. usually good
                                                         no CPU required (sort of)

   Cons        requires CPU              HARD!              more assumptions
               non-deterministic    not always known            not exact 
--------------------------------------------------------------------------------

## Example \#2: two categorical variables

- Simulation
    - randomization test
- Exact
    - Fisher's exact test, hypergeometric 
- Approximation
    - $\chi^2$-test of independence

# How did I get here?

## When I took stats

- ~~Simulation~~
- ~~Exact~~ (in probability class)
- Approximation

## It's not just me...



<div class="double">
<p class="double-flow">
- <img src="https://andrewpbray.github.io/abray-pic2.JPG" width="150px">
Andrew Bray
- <img src="https://www.causeweb.org/sbi/wp-content/uploads/2015/03/GCobb.png" width="150px">
 George Cobb
</p>

<p class="double-flow">
- <img src="https://web.stanford.edu/~hastie/CASI/images/jacket_wave_cropped.jpg" width="200px">
Brad Efron
</p>
</div>

## DataCamp

<div class="double">
<p class="double-flow">
- [Statistics with R](https://www.datacamp.com/tracks/statistics-with-r) skills track
- 4 courses available now
- 4 more courses available soon

</p>
<p class="double-flow">
  <img src="datacamp.png" width="300px">
</p>
</div>


## Pictures { .fullpage }

<img src="statistical_inference.jpg" class="white" width="600px">

# Unification?

## Common setup with `infer`

```{r, error=TRUE}
# devtools::install_github("andrewpbray/infer")
library(infer)

fake_null <- data_frame(
  candidate = rep(c("clinton", "trump"), each = n/2)
)

setup <- fake_null %>%
  specify(response = candidate) %>%
  hypothesize(null = "point", p = c("clinton" = 0.5, "trump" = 0.5))
```

## Method \#1 with `infer`

```{r, fig.height=3}
setup %>%
  generate(reps = 1000, type = "simulate") %>%
  calculate(stat = "prop") %>%
  visualize() + 
  geom_vline(data = p_hat, aes(xintercept = clinton_pct), 
             color = "gold", size = 2, linetype = 2)
```

## Method \#3 with `infer`

```{r, eval=FALSE}
setup %>%
  calculate(stat = "prop") %>%
  visualize() + 
  geom_vline(data = p_hat, aes(xintercept = clinton_pct), 
             color = "gold", size = 2, linetype = 2)
```

## Thank you! {.fullpage}

<img src="https://media.giphy.com/media/aAW7yJ4m7YCti/giphy.gif" width="400px">

</br></br></br></br></br></br>
(https://github.com/mine-cetinkaya-rundel/novel-first-stat-ds-jsm2017)
