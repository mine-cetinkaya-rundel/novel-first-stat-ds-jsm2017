---
title: "Three Methods for Statistical Inference"
author: "Ben Baumer"
date: "July 31, 2017"
output:
  rmdshower::shower_presentation:
    self_contained: false
    katex: true
    ratio: 16x10
abstract: "Statistics educators---imbued with improved computing power---have advocated for a greater emphasis on randomization and simulation-based techniques for statistical inference in recent years. While these ideas are not new, the traditional treatment of inference in introductory statistics courses has focused on methods that approximate the sampling distribution of a statistic with a probability distribution. We describe an approach to teaching inference in an introductory statistics course---for students who primarily major in the sciences---that emphasizes randomization and simulation-based approaches, briefly discusses but largely glosses over mathematical approaches using probability theory, and treats normal-based approximations as an alternative technique. The overall conceptual goal is to understand the sampling distribution of the test statistic: the three approaches are just means to that end. "
---

## Three Methods for Statistical Inference { .white }

<img src="http://1.bp.blogspot.com/-ueOnJA9cGJk/VHWza-0saaI/AAAAAAAAP4g/WTbdP2qNSaE/s1920/monkey-wallpaper-hd.jpg" class="cover">

</br></br></br>
<p class="white">
Ben Baumer</br>
JSM Baltimore</br>
July 31st, 2017</br>
(https://github.com/mine-cetinkaya-rundel/novel-first-stat-ds-jsm2017)
</p>

```{r, echo=FALSE}
knitr::opts_chunk$set(fig.height = 3.5)
```

## Three Methods to constuct null distribution

> 1. Simulation: use computer to *simulate* 
> 2. Exact: use math to *compute*
> 3. Approximation: use statistical theory to *approximate*

# An example:</br>one proportion

## Method \#1: simulation


```{r, message=FALSE}
library(tidyverse)

outcomes <- data_frame(candidate = c("clinton", "trump"))
p_0 <- 1/2

# http://www.cnn.com/election/results/exit-polls
n <- 2458

sim <- outcomes %>%
  oilabs::rep_sample_n(size = n, replace = TRUE, reps = 1000) %>%
  group_by(replicate) %>%
  summarize(N = n(), 
            clinton_votes = sum(candidate == "clinton")) %>%
  mutate(clinton_pct = clinton_votes / N)
```

## Method \#1: simulation, observed

```{r}
# observed proportion
p_hat <- data_frame(clinton_pct = 0.5092953)

observed <- ggplot(data = p_hat) + 
  geom_vline(aes(xintercept = clinton_pct), 
             color = "gold", size = 2, linetype = 2) + 
  scale_x_continuous(limits = c(0.45, 0.55))
```

## Method \#1: simulation, plot

```{r, message=FALSE}
observed + 
  geom_density(data = sim, aes(x = clinton_pct))
```

## Method \#2: exact

- Let $X \sim Bernoulli(p_0)$, then
    - $\mathbb{E}[X] = p_0, \qquad Var[X] = p_0 (1 - p_0)$
    
- Let $Z = \frac{X_1 + \cdots + X_n}{n}$, then
    - $\mathbb{E}[Z] = p_0, \qquad Var[Z] = \frac{p_0 (1 - p_0)}{n}$
    - for later, $sd(Z) = \sqrt{\frac{p_0 (1 - p_0)}{n}}$



## Method \#2: exact, plot

```{r, fig.height=3}
dbinom_p <- function (x, size, prob, log = FALSE) {
  n * dbinom(round(x * size), size, prob, log)
}
observed +
  stat_function(fun = dbinom_p, args = c(size = n, prob = p_0))
```

## Method \#3: approximation

- For $np > 10$ and $n(1-p) > 10$,
$$Binomial(n, p) \approx Normal \left( p_0, \sqrt{\frac{p_0 (1 - p_0)}{n}} \right) $$

```{r}
se_p0 <- sqrt(p_0 * (1-p_0) / n)
```

## Method \#3: approximation, plot

```{r}
observed +
  stat_function(fun = dnorm, args = c(mean = p_0, sd = se_p0))
```

## Three methods comparison, plot

```{r}
observed +
  geom_density(data = sim, aes(x = clinton_pct)) + 
  stat_function(fun = dbinom_p, args = c(size = n, prob = p_0), color = "cyan") +
  stat_function(fun = dnorm, args = c(mean = p_0, sd = se_p0), color = "magenta")
```

## Three methods comparison, table

--------------------------------------------------------------------------------
             Simulation                    Exact             Approximation
----------- --------------------- --------------------- ------------------------
Assumptions     independence           independence          independence
                                     probability model        normality
                                                              $np > 10$, etc.

  Pros          no math               exact solution          uses normal
                flexible                                   approx. usually good
                                                         no CPU required (sort of)

   Cons        requires CPU              HARD!              more assumptions
               non-deterministic                            not exact 
--------------------------------------------------------------------------------

# How did I get here?

## When I took stats

- ~~Simulation~~
- ~~Exact~~ (in probability class)
- Approximation

## Contributions from



<div class="double">
<p class="double-flow">
- George Cobb
- Andrew Bray
- Brad Efron

</p><p class="double-flow">

<img src="https://www.causeweb.org/sbi/wp-content/uploads/2015/03/GCobb.png" width="200px">
<img src="https://andrewpbray.github.io/abray-pic2.JPG" width="200px">
<img src="https://web.stanford.edu/~hastie/CASI/images/jacket_wave_cropped.jpg" width="200px">
</p>
</div>

## DataCamp

- Mine Cetinkaya-Rundel
- Andrew Bray
- Jo Hardin

## Pictures { .fullpage }

<img src="statistical_inference.jpg" class="white" width="600px">

# Unification?

## Common setup with `infer`

```{r, error=TRUE}
# devtools::install_github("andrewpbray/infer")
library(infer)

fake_null <- data_frame(
  candidate = rep(c("clinton", "trump"), each = n/2)
)

setup <- fake_null %>%
  specify(response = candidate) %>%
  hypothesize(null = "point", p = c("clinton" = 0.5, "trump" = 0.5))
```

## Method \#1 with `infer`

```{r, fig.height=3}
setup %>%
  generate(reps = 1000, type = "simulate") %>%
  calculate(stat = "prop") %>%
  visualize() + 
  geom_vline(data = p_hat, aes(xintercept = clinton_pct), 
             color = "gold", size = 2, linetype = 2)
```

## Method \#3 with `infer`

```{r, eval=FALSE}
setup %>%
  calculate(stat = "prop") %>%
  visualize() + 
  geom_vline(data = p_hat, aes(xintercept = clinton_pct), 
             color = "gold", size = 2, linetype = 2)
```

## Thank you! {.fullpage}

<img src="https://media.giphy.com/media/aAW7yJ4m7YCti/giphy.gif" width="400px">

</br></br></br></br></br></br>
(https://github.com/mine-cetinkaya-rundel/novel-first-stat-ds-jsm2017)
